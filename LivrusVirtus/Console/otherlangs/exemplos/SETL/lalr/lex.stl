--
--  Lexical Analyzer
--  ================
--
--  This package contains the lexical analyzer for an LALR parse table
--  generator.  Compared to the lexical analyzer of a compiler, this is a
--  mess.  The problem is that we don't want to predefine any more
--  symbols than absolutely necessary, because any symbol we use is one
--  the user may not use as a grammar symbol.  To combat this problem,
--  this lexical analyzer uses a trie of special symbols, which can be
--  easily changed.
--
--  To further complicate matters, we have to recognize and output
--  semantic actions while we're scanning.  There are also user-definable
--  special tokens in these things, so we use another trie.
--
--  The caller has a great deal of control over the symbol trie, but very
--  little over the semantic action trie, since only the lexical analyzer
--  pays any attention to the semantic actions.
--

package lexical_analyzer;

   --
   --  Tokens are tuples, in the following format
   --

   sel tok_class(1),                   -- token class
       tok_lexeme(2),                  -- token lexeme
       tok_line(3),                    -- source line number
       tok_column(4);                  -- source column number

   procedure reset_break_strings();
   procedure init_lex(source_file_name);
   procedure get_token();
   procedure write_semantic_action(line_number, action);
   procedure error_message(message, line, column);

end lexical_analyzer;

package body lexical_analyzer;

   use globals,                        -- global symbols
       token_classes;                  -- LALR output token tables

   const id_cset := "abcdefghijklmnopqrstuvwxyz" +
                    "ABCDEFGHIJKLMNOPQRSTUVWXYZ" +
                    "0123456789_";

   var source_handle := om,
       line_buffer,
       line_number,
       line_length,
       symbol_trie,
       symbol_break_string,
       comment_break_string,
       action_symbol_trie,
       action_symbol_break,
       trie_terminal;

   --
   --  reset_break_strings
   --  -------------------
   --
   --  This procedure is an uncomfortable gimmick.  We allow the
   --  programmer to set his own comment delimiters, as a convenience.
   --  It makes life much easier within text editors if the comment style
   --  used by the grammar is the same as that used by the semantic
   --  actions.  This is the ONLY special symbol we let him control,
   --  unlike Phillippe, who lets the programmer control everything.
   --
   --  We use strings for span and break to speed up scanning in several
   --  places, and the break strings (really character sets) depend upon
   --  the comment delimiters.  We reset them here, to avoid resetting on
   --  every call to get_token.  This forces us to call this procedure
   --  everywhere these things can change (Sigh).
   --

   procedure reset_break_strings();

      symbol_trie := add_to_trie({},special_map);
      symbol_trie := add_to_trie(symbol_trie,
                                 {[escape_mark + lexeme, tclass] :
                                  [lexeme, tclass] in keyword_map});
      symbol_break := " \t"+/[c : c in domain(symbol_trie)];
      comment_break_string := +/[d(1) : d in comment_delimiters | #d > 0];
      action_symbol_trie := add_to_trie({}, {
            [escape_mark + "rule_text",  [rule_text_string, []] ],
            [escape_mark + "rule",       [rule_string, []] ] });
      action_symbol_break := +/[c : c in domain(action_symbol_trie)];

   end reset_break_strings;

   --
   --  init_lex
   --  --------
   --
   --  This procedure initializes the lexical analyzer.  It opens the
   --  source and action files, aborting if it can't, and fills the line
   --  buffer.
   --

   procedure init_lex(source_file_name);

      --
      --  Set up the symbol trie, from the token tables generated by
      --  LALR.
      --

      trie_terminal := newat();
      reset_break_strings();

      --
      --  Open the source file, abort if we can't.
      --

      if source_handle /= om then
         close(source_handle);
         source_handle := om;
      end if;
      source_handle := open(source_file_name, "text-in");
      if source_handle = om then
         print("Can not open ", source_file_name);
         stop;
      end if;
      if action_handle /= om then
         close(action_handle);
         action_handle := om;
      end if;

      --
      --  Fill the input buffers.
      --

      geta(source_handle, line_buffer);
      line_number := 1;
      line_length := if line_buffer /= om then #line_buffer else 0 end if;
      token_buffer := [];
      pending_actions := [];

   end init_lex;

   --
   --  get_token
   --  ---------
   --

   procedure get_token();

      var comment_nesting_level := 0;  -- number of open comment delimiters

      --
      --  Loop until we find something to return.
      --

      loop
         
         --
         --  If the file is empty now, return an end of file marker.
         --

         if line_buffer = om then

            if source_handle /= om then
               close(source_handle);
               source_handle := om;
            end if;
            return [tok_eof, "$EOF", line_number, 1];

         end if;
         
         --
         --  Strip away any leading blanks and read another record if
         --  there isn't anything left of the current one.
         --

         span(line_buffer, " \t");
         if #line_buffer = 0 then

            geta(source_handle, line_buffer);
            line_number +:= 1;
            line_length := 
               if line_buffer /= om then #line_buffer else 0 end if;
            continue;

         end if;

         --
         --  It's conceivable we could be inside a comment.  We strip
         --  away stuff until we find a balanced comment end.
         --

         if comment_nesting_level > 0 then

            break(line_buffer,comment_break_string);
            if line_buffer := "" then
               continue;
            elseif match(line_buffer,comment_delimiters(1)) /= "" then
               comment_nesting_level +:= 1;
               continue;
            elseif match(line_buffer,comment_delimiters(2)) /= "" then
               comment_nesting_level -:= 1;
               continue;
            else
               line_buffer := line_buffer(2..);
               continue;
            end if;

         end if;
        
         --
         --  Next we strip away comments.  We expect the global variable
         --  comment_delmiters to contain a pair, the first element being
         --  the beginning of comment mark, and the second either the end
         --  or omega, in which case we recognize prefix comment marks,
         --  as in SETL.
         --

         if match(line_buffer,comment_delimiters(1)) /= "" then

            if comment_delimiters(2) = om or comment_delimiters(2) = "" then
               line_buffer := "";
            else
               comment_nesting_level := 1;
            end if;   
            continue;

         end if;

         --
         --  Gather up semantic actions and place them on a list.
         --

         if match(line_buffer, "/.") /= "" then

            test_line_buffer := line_buffer;
            span(test_line_buffer, " \t");
            if test_line_buffer = "" then
              
               geta(source_handle, line_buffer);
               if eof() then
                  print("Missing ./ starting at line ",
                        action_line_number);
                  stop;
               end if;

               line_number +:= 1;
               line_length := #line_buffer;

            end if;

            action := [];
            action_line_number := line_number;
           
            loop

               if "./" in line_buffer then

                  action_line := "";
                  loop

                     action_line +:= break(line_buffer, ".");   
                     exit when match(line_buffer, "./") /= "";
                     action_line +:= junk fromb line_buffer;

                  end loop;

                  action with:= action_line;
                  exit;

               else

                  action with:= line_buffer;

               end if;

               geta(source_handle, line_buffer);
               if eof() then
                  print("Missing ./ starting at line ",
                        action_line_number);
                  stop;
               end if;

               line_number +:= 1;
               line_length := #line_buffer;

            end loop;

            --
            --  Save the action on the pending actions list.
            --

            if error_count = 0 then
               pending_actions with:= [action_line_number, action];
            end if;
            continue;

         end if;

         --
         --  At this point we should be looking at the first character of
         --  a token we will wish to return.
         --

         case line_buffer(1)

            --
            --  Keywords and identifiers follow normal programming
            --  language rules.  They begin with a letter, followed by a
            --  sequence of letters, numbers and underscores.
            --

            when "a","b","c","d","e","f","g","h","i","j","k","l","m",
                 "n","o","p","q","r","s","t","u","v","w","x","y","z",
                 "A","B","C","D","E","F","G","H","I","J","K","L","M",
                 "N","O","P","Q","R","S","T","U","V","W","X","Y","Z","_" =>

               token_column := line_length - #line_buffer + 1;

               lexeme := span(line_buffer,id_cset);
               
               return [keyword_map(lexeme) ? tok_identifier,
                       lexeme, line_number, token_column]; 

            --
            --  Quoted strings allow us to reuse things which look like
            --  reserved words or special symbols.  The quotes are not
            --  considered part of the string.  Embedded quotes are
            --  included by the \" notation, but this is the only escape
            --  sequence recognized.
            --

            when "\"" =>

               token_column := line_length - #line_buffer + 1;

               line_buffer := line_buffer(2..);
               lexeme := "";
               loop

                  next_substring := break(line_buffer,"\\\"");
                  lexeme +:= next_substring;
                  if #line_buffer = 0 then

                     error_message("Missing closing quote at => "+lexeme,
                                   line_number,token_column);
                     exit;            

                  elseif line_buffer(1) = "\"" then
                              
                     line_buffer := line_buffer(2..);
                     return [tok_identifier, 
                             lexeme, line_number, token_column];

                  else -- first char is \

                     if match(line_buffer, "\\\"") /= "" then
                        lexeme +:= "\"";
                        continue;
                     else
                        error_message("Invalid escape sequence at => "+lexeme,
                                       line_number,token_column);
                     end if;

                  end if;

               end loop;

            --
            --  We've gathered all the special symbols into a trie, so
            --  that it's easy to change.  We scan until we find the
            --  longest string in the trie and return the associated
            --  token class.
            --

            otherwise =>

               token_column := line_length - #line_buffer + 1;
               found_symbol := om;

               trie := symbol_trie;
               i := 1;
               while i <= #line_buffer and trie(line_buffer(i)) /= om loop

                  if trie(trie_terminal) /= om then
                     found_symbol := [line_buffer(1 .. i - 1), 
                                      trie(trie_terminal)];
                  end if;
                  trie := trie(line_buffer(i));
                  i +:= 1;

               end loop;

               if trie(trie_terminal) /= om then
                  found_symbol := [line_buffer(1 .. i - 1), 
                                   trie(trie_terminal)];
               end if;

               if found_symbol /= om then

                  line_buffer := line_buffer(#found_symbol(1) + 1 ..);
                  [lexeme, token_class] := found_symbol;
                  return [token_class, lexeme, line_number, token_column];

               end if;

               error_message("Invalid token at => "+line_buffer,
                             line_number,token_column);
               line_buffer := "";

               continue;

         end case;

      end loop;            

   end get_token;

   --
   --  write_semantic_action
   --  ---------------------
   --
   --  This procedure accepts a semantic action picked off by get_token
   --  and writes it to the semantic action file.  We use a trie of
   --  special symbols and associated actions to provide a kind of macro
   --  facility.  For the time being we only have the two default
   --  macros.
   --

   procedure write_semantic_action(line_number, action);

      if error_count > 0 then
         action_symbol_break := "";
      end if;

      if action_handle = om and
         (action_handle := open(action_file_name, "text-out")) = om then
       
         print("Unable to open action file => ", action_file_name);
         stop;
   
      end if;

      --
      --  Write the C line number, if requested.
      --

      if cpp_line_numbers then
         printa(action_handle, "#line ", line_number,
                              " \"", source_file_name, "\"");
      end if;
         
      --
      --  Delete the last line, if it's empty.
      --

      if action(#action) = " " * #action(#action) then
         action := action(1..#action - 1);
      end if;

      for action_line in action loop

         output_line := "";
         loop

            output_line +:= break(action_line, action_symbol_break);
            exit when #action_line = 0;
            
            possible_symbol := "";
            found_symbol := om;

            trie := action_symbol_trie;
            i := 1;
            while i <= #action_line and trie(action_line(i)) /= om loop

               if trie(trie_terminal) /= om then
                  found_symbol := [action_line(1 .. i - 1),
                                   trie(trie_terminal)];
               end if;

               trie := trie(action_line(i));
               i +:= 1;

            end loop;

            if trie(trie_terminal) /= om then
               found_symbol := [action_line(1 .. i - 1),
                                trie(trie_terminal)];
            end if;

            if found_symbol /= om then
               action_line := action_line(#found_symbol(1) + 1 ..);
               output_line +:= (found_symbol(2)(1))(found_symbol(2)(2));
            else
               output_line +:= junk fromb action_line;
            end if;

         end loop;

         printa(action_handle, output_line);

      end loop;

   end write_semantic_action;

   --
   --  rule_text_string
   --  ----------------
   --
   --  This procedure makes a printable string from the current rule.
   --

   procedure rule_text_string(args);

      return symbol_list(current_lhs) + " " +
             "::=" 
             +/[" " + if s = 0 then
                         escape_mark + "epsilon"
                      else symbol_list(s) end if :
                s in current_rhs];

   end rule_text_string;

   --
   --  rule_string
   --  -----------
   --
   --  This procedure makes a printable string from the current rule
   --  number.
   --

   procedure rule_string(args);

      return str(num_rules);

   end rule_string;

   --
   --  add_to_trie
   --  -----------
   --
   --  This procedure provides an easy way to construct a trie as the
   --  lexical analyzer wants it.  It calls the recursive procedure
   --  insert_symbol on each symbol it finds.  Insert_symbol handles one
   --  character, and calls itself on the tail of the symbol.  We assume
   --  all symbols are case insensitive.
   --

   procedure add_to_trie(trie, symbol_list);

      for [symbol, symbol_value] in symbol_list loop 
         trie := insert_symbol(trie, symbol, symbol_value);
      end loop;

      return trie;

      procedure insert_symbol(trie, symbol, symbol_value);

         if #symbol = 0 then

            trie(trie_terminal) := symbol_value;
            return trie;

         else

            trie(symbol(1)) := insert_symbol(trie(symbol(1)) ? {},
                                             symbol(2..), symbol_value);

            if (other := tolower_map(symbol(1))) /= om or
               (other := toupper_map(symbol(1))) /= om then
               trie(other) := trie(symbol(1));
            end if;

            return trie;

         end if;

      end insert_symbol;
      
   end add_to_trie;

   --
   --  error_message
   --  -------------
   --
   --  This procedure prints an error message on standard output, and
   --  keeps a running total.
   --

   procedure error_message(message, line, column);

      print(rpad("["+str(line)+":"+str(column)+"]", 9),
            "*ERROR* => ", message);
      error_count +:= 1;

   end;

end lexical_analyzer;
